{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geopandas\n",
    "import rioxarray\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# To avoid warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "filename: GL_TS_MO_4400050.nc\n",
      "<xarray.Dataset>\n",
      "Dimensions:       (TIME: 3593, LATITUDE: 173823, LONGITUDE: 173823,\n",
      "                   POSITION: 173823, DEPTH: 2)\n",
      "Coordinates:\n",
      "  * TIME          (TIME) datetime64[ns] 2020-10-14T13:09:59.999999744 ... 202...\n",
      "  * LATITUDE      (LATITUDE) float32 46.7 46.7 46.7 46.7 ... 46.7 46.7 46.7 46.7\n",
      "  * LONGITUDE     (LONGITUDE) float32 -56.18 -56.18 -56.18 ... -56.18 -56.18\n",
      "Dimensions without coordinates: POSITION, DEPTH\n",
      "Data variables: (12/31)\n",
      "    TIME_QC       (TIME) float32 ...\n",
      "    POSITION_QC   (POSITION) float32 ...\n",
      "    DC_REFERENCE  (TIME) object ...\n",
      "    DEPH          (TIME, DEPTH) float32 ...\n",
      "    DEPH_QC       (TIME, DEPTH) float32 ...\n",
      "    TEMP          (TIME, DEPTH) float64 ...\n",
      "    ...            ...\n",
      "    VTM02         (TIME, DEPTH) float64 ...\n",
      "    VTM02_QC      (TIME, DEPTH) float32 ...\n",
      "    VTPK          (TIME, DEPTH) float64 ...\n",
      "    VTPK_QC       (TIME, DEPTH) float32 ...\n",
      "    VZMX          (TIME, DEPTH) float64 ...\n",
      "    VZMX_QC       (TIME, DEPTH) float32 ...\n",
      "Attributes: (12/49)\n",
      "    data_type:                      OceanSITES time-series data\n",
      "    format_version:                 1.4\n",
      "    platform_code:                  4400050\n",
      "    institution:                    Centre for Studies on Risks Environment M...\n",
      "    institution_edmo_code:          4533 4627\n",
      "    site_code:                       \n",
      "    ...                             ...\n",
      "    last_date_observation:          2023-11-30T19:30:00Z\n",
      "    last_latitude_observation:      46.70000\n",
      "    last_longitude_observation:     -56.18333\n",
      "    date_update:                    2023-12-01T15:05:23Z\n",
      "    history:                        2023-12-01T15:05:23Z : Creation\n",
      "    data_mode:                      R\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "parent_data_dir = r'C:/Users/s1834371/Documents/PLUMES/validation/mooring_test_4/'  #file directory of in-situ data\n",
    "\n",
    "# Count number of files to list through\n",
    "file_list = []\n",
    "for (dir_path, dir_names, file_names) in os.walk(os.path.normpath(parent_data_dir)):\n",
    "        file_list.extend(file_names)\n",
    "    \n",
    "len_files = len(file_list) #length of list of all files\n",
    "count = 1 #for loop code\n",
    "file_dict = {}\n",
    "file_dict['directory'] = []\n",
    "file_dict['tile'] = []\n",
    "\n",
    "for path, subdirs, files in os.walk(os.path.normpath(parent_data_dir)):\n",
    "    for name in files:\n",
    "        if name.endswith(\".nc\"):\n",
    "            print('\\n====================')\n",
    "            print('filename:', name)\n",
    "\n",
    "            insitu_data = xr.open_dataset(os.path.join(path, name))  # open in-situ data\n",
    "            insitu_data = insitu_data.sel(TIME=slice('2020-01-01', '2020-12-31')) # select time period of interest\n",
    "            print(insitu_data)\n",
    "\n",
    "            ##### looping through each timestep and retrieving lat, lon, time, depth and temperature data #####\n",
    "            for index in range(len(insitu_data.TIME)):\n",
    "                print('timestep:', index+1, 'of', len(insitu_data.TIME))\n",
    "                insitu_data_t = insitu_data.isel(TIME=index) #select time\n",
    "                insitu_data_t = insitu_data_t.isel(LONGITUDE=index) #select longitude\n",
    "                insitu_data_t = insitu_data_t.isel(LATITUDE=index) #select latitude\n",
    "                insitu_data_t = insitu_data_t.isel(POSITION=index) #select position\n",
    "                insitu_data_t.load() # solves the issue but makes it a lot slower\n",
    "\n",
    "                if 'DC_REFERENCE' in insitu_data_t.variables:\n",
    "                    insitu_data_t = insitu_data_t.drop_vars(['DC_REFERENCE']) # drop unnecessary variables - required to be able to subset to depth\n",
    "                if 'DC_REFERENCE' in insitu_data_t.variables:\n",
    "                    insitu_data_t = insitu_data_t.drop_vars(['DIRECTION'])  # drop unnecessary variables\n",
    "\n",
    "                # for var in insitu_data_t.variables:\n",
    "                #     print(insitu_data_t[var].dtype, var)\n",
    "                insitu_data_upper_3m_t = insitu_data_t.where((insitu_data_t['DEPH'] >= 0.0) & (insitu_data_t['DEPH'] < 3.0), drop=True)\n",
    "                # print(insitu_data_upper_3m_t.DEPH_QC.values)\n",
    "                # print(insitu_data_upper_3m_t.TIME_QC.values)\n",
    "                # sys.exit()\n",
    "      \n",
    "  \n",
    "\n",
    "                try: # try to subsetting to upper 3m, if no data exsists move on to next timestep\n",
    "                    insitu_data_upper_3m_t = insitu_data_upper_3m_t.where((insitu_data_upper_3m_t['TEMP_QC'] == 1) & (insitu_data_upper_3m_t['DEPH_QC'] == 1) | (insitu_data_upper_3m_t['DEPH_QC'] == 7) & (insitu_data_upper_3m_t['TIME_QC'] <= 2) & (insitu_data_upper_3m_t['POSITION_QC'] <= 2), drop=True)  # good data only     \n",
    "                    #insitu_data_upper_3m_t = insitu_data_upper_3m_t.where((insitu_data_upper_3m_t['DEPH'] == insitu_data_upper_3m_t['DEPH'].min()), drop=True) #select only the uppermost depth\n",
    "                    insitu_data_upper_3m_t['TEMP'] = insitu_data_upper_3m_t['TEMP'].mean() # take the mean of the temperature values\n",
    "\n",
    "                    insitu_df = insitu_data_upper_3m_t.to_dataframe() # convert to pandas dataframe for saving\n",
    "                    insitu_df['ID'] = insitu_data_upper_3m_t.attrs['id'] # add ID variable\n",
    "\n",
    "                    if index == 0: # if not the first timestep append to dataframe\n",
    "                        insitu_df_all = insitu_df\n",
    "                    else:\n",
    "                        insitu_df_all = insitu_df_all.append(insitu_df)\n",
    "                \n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            try:\n",
    "                insitu_df_all.to_csv(os.path.join(path, name[:-3] + '_surface.csv')) # save as csv\n",
    "            except:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "filename: GL_TS_MO_4400050.nc\n",
      "<xarray.Dataset>\n",
      "Dimensions:       (TIME: 3593, LATITUDE: 173823, LONGITUDE: 173823,\n",
      "                   POSITION: 173823, DEPTH: 2)\n",
      "Coordinates:\n",
      "  * TIME          (TIME) datetime64[ns] 2020-10-14T13:09:59.999999744 ... 202...\n",
      "  * LATITUDE      (LATITUDE) float32 46.7 46.7 46.7 46.7 ... 46.7 46.7 46.7 46.7\n",
      "  * LONGITUDE     (LONGITUDE) float32 -56.18 -56.18 -56.18 ... -56.18 -56.18\n",
      "Dimensions without coordinates: POSITION, DEPTH\n",
      "Data variables: (12/31)\n",
      "    TIME_QC       (TIME) float32 ...\n",
      "    POSITION_QC   (POSITION) float32 ...\n",
      "    DC_REFERENCE  (TIME) object ...\n",
      "    DEPH          (TIME, DEPTH) float32 ...\n",
      "    DEPH_QC       (TIME, DEPTH) float32 ...\n",
      "    TEMP          (TIME, DEPTH) float64 ...\n",
      "    ...            ...\n",
      "    VTM02         (TIME, DEPTH) float64 ...\n",
      "    VTM02_QC      (TIME, DEPTH) float32 ...\n",
      "    VTPK          (TIME, DEPTH) float64 ...\n",
      "    VTPK_QC       (TIME, DEPTH) float32 ...\n",
      "    VZMX          (TIME, DEPTH) float64 ...\n",
      "    VZMX_QC       (TIME, DEPTH) float32 ...\n",
      "Attributes: (12/49)\n",
      "    data_type:                      OceanSITES time-series data\n",
      "    format_version:                 1.4\n",
      "    platform_code:                  4400050\n",
      "    institution:                    Centre for Studies on Risks Environment M...\n",
      "    institution_edmo_code:          4533 4627\n",
      "    site_code:                       \n",
      "    ...                             ...\n",
      "    last_date_observation:          2023-11-30T19:30:00Z\n",
      "    last_latitude_observation:      46.70000\n",
      "    last_longitude_observation:     -56.18333\n",
      "    date_update:                    2023-12-01T15:05:23Z\n",
      "    history:                        2023-12-01T15:05:23Z : Creation\n",
      "    data_mode:                      R\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "parent_data_dir = r'C:/Users/s1834371/Documents/PLUMES/validation/mooring_test_4/'  #file directory of in-situ data\n",
    "\n",
    "# Count number of files to list through\n",
    "file_list = []\n",
    "for (dir_path, dir_names, file_names) in os.walk(os.path.normpath(parent_data_dir)):\n",
    "        file_list.extend(file_names)\n",
    "    \n",
    "len_files = len(file_list) #length of list of all files\n",
    "count = 1 #for loop code\n",
    "file_dict = {}\n",
    "file_dict['directory'] = []\n",
    "file_dict['tile'] = []\n",
    "\n",
    "for path, subdirs, files in os.walk(os.path.normpath(parent_data_dir)):\n",
    "    for name in files:\n",
    "        if name.endswith(\".nc\"):\n",
    "            print('\\n====================')\n",
    "            print('filename:', name)\n",
    "\n",
    "            insitu_data = xr.open_dataset(os.path.join(path, name))  # open in-situ data\n",
    "            insitu_data = insitu_data.sel(TIME=slice('2020-01-01', '2020-12-31')) # select time period of interest\n",
    "            print(insitu_data)\n",
    "\n",
    "            ##### looping through each timestep and retrieving lat, lon, time, depth and temperature data #####\n",
    "            for index in range(len(insitu_data.TIME)):\n",
    "                print('timestep:', index+1, 'of', len(insitu_data.TIME))\n",
    "                insitu_data_t = insitu_data.isel(TIME=index) #select time\n",
    "                insitu_data_t = insitu_data_t.isel(LONGITUDE=index) #select longitude\n",
    "                insitu_data_t = insitu_data_t.isel(LATITUDE=index) #select latitude\n",
    "                insitu_data_t = insitu_data_t.isel(POSITION=index) #select position\n",
    "                insitu_data_t.load() # solves the issue but makes it a lot slower\n",
    "\n",
    "                if 'DC_REFERENCE' in insitu_data_t.variables:\n",
    "                    insitu_data_t = insitu_data_t.drop_vars(['DC_REFERENCE']) # drop unnecessary variables - required to be able to subset to depth\n",
    "                if 'DC_REFERENCE' in insitu_data_t.variables:\n",
    "                    insitu_data_t = insitu_data_t.drop_vars(['DIRECTION'])  # drop unnecessary variables\n",
    "\n",
    "                # for var in insitu_data_t.variables:\n",
    "                #     print(insitu_data_t[var].dtype, var)\n",
    "                insitu_data_upper_3m_t = insitu_data_t.where((insitu_data_t['DEPH'] >= 0.0) & (insitu_data_t['DEPH'] < 3.0), drop=True)\n",
    "                # print(insitu_data_upper_3m_t.DEPH_QC.values)\n",
    "                # print(insitu_data_upper_3m_t.TIME_QC.values)\n",
    "                # sys.exit()\n",
    "      \n",
    "  \n",
    "\n",
    "                try: # try to subsetting to upper 3m, if no data exsists move on to next timestep\n",
    "                    insitu_data_upper_3m_t = insitu_data_upper_3m_t.where((insitu_data_upper_3m_t['TEMP_QC'] == 1) & (insitu_data_upper_3m_t['DEPH_QC'] == 1) | (insitu_data_upper_3m_t['DEPH_QC'] == 7) & (insitu_data_upper_3m_t['TIME_QC'] <= 2) & (insitu_data_upper_3m_t['POSITION_QC'] <= 2), drop=True)  # good data only     \n",
    "                    #insitu_data_upper_3m_t = insitu_data_upper_3m_t.where((insitu_data_upper_3m_t['DEPH'] == insitu_data_upper_3m_t['DEPH'].min()), drop=True) #select only the uppermost depth\n",
    "                    insitu_data_upper_3m_t['TEMP'] = insitu_data_upper_3m_t['TEMP'].mean() # take the mean of the temperature values\n",
    "\n",
    "                    insitu_df = insitu_data_upper_3m_t.to_dataframe() # convert to pandas dataframe for saving\n",
    "                    insitu_df['ID'] = insitu_data_upper_3m_t.attrs['id'] # add ID variable\n",
    "\n",
    "                    if index == 0: # if not the first timestep append to dataframe\n",
    "                        insitu_df_all = insitu_df\n",
    "                    else:\n",
    "                        insitu_df_all = insitu_df_all.append(insitu_df)\n",
    "                \n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            try:\n",
    "                insitu_df_all.to_csv(os.path.join(path, name[:-3] + '_surface.csv')) # save as csv\n",
    "            except:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import rioxarray as rio\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "\n",
    "def ROI_select(DATA, ROI_dir, invert=None): # clips raster to given shapefile area, option to invert or not\n",
    "        print('Clipping DATA to ROI')\n",
    "        shapefile_dir = ROI_dir\n",
    "        glacier_shape = geopandas.read_file(shapefile_dir)\n",
    "        DATA.rio.set_spatial_dims(x_dim=\"LONGITUDE\", y_dim=\"LATITUDE\", inplace=True)\n",
    "        DATA.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "        DATA = DATA.rio.clip(glacier_shape.geometry.apply(mapping), glacier_shape.crs, drop=True, all_touched=True, invert=invert)\n",
    "        return DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 61961 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "filename: GL_TS_MO_4400050.nc\n",
      "<xarray.Dataset>\n",
      "Dimensions:      (TIME: 173823, POSITION: 173823, LONGITUDE: 173823,\n",
      "                  LATITUDE: 173823)\n",
      "Coordinates:\n",
      "  * LONGITUDE    (LONGITUDE) float32 -56.18 -56.18 -56.18 ... -56.18 -56.18\n",
      "  * LATITUDE     (LATITUDE) float32 46.7 46.7 46.7 46.7 ... 46.7 46.7 46.7 46.7\n",
      "  * TIME         (TIME) datetime64[ns] 2011-11-30T14:39:59.999999744 ... 2023...\n",
      "  * POSITION     (POSITION) int64 0 1 2 3 4 ... 173819 173820 173821 173822\n",
      "Data variables:\n",
      "    TEMP         (TIME) float64 dask.array<chunksize=(17382,), meta=np.ndarray>\n",
      "    TEMP_QC      (TIME) float32 dask.array<chunksize=(17382,), meta=np.ndarray>\n",
      "    DEPH         (TIME) float32 dask.array<chunksize=(17382,), meta=np.ndarray>\n",
      "    DEPH_QC      (TIME) float32 dask.array<chunksize=(17382,), meta=np.ndarray>\n",
      "    TIME_QC      (TIME) float32 dask.array<chunksize=(17382,), meta=np.ndarray>\n",
      "    POSITION_QC  (POSITION) float32 dask.array<chunksize=(17382,), meta=np.ndarray>\n",
      "Attributes: (12/49)\n",
      "    data_type:                      OceanSITES time-series data\n",
      "    format_version:                 1.4\n",
      "    platform_code:                  4400050\n",
      "    institution:                    Centre for Studies on Risks Environment M...\n",
      "    institution_edmo_code:          4533 4627\n",
      "    site_code:                       \n",
      "    ...                             ...\n",
      "    last_date_observation:          2023-11-30T19:30:00Z\n",
      "    last_latitude_observation:      46.70000\n",
      "    last_longitude_observation:     -56.18333\n",
      "    date_update:                    2023-12-01T15:05:23Z\n",
      "    history:                        2023-12-01T15:05:23Z : Creation\n",
      "    data_mode:                      R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\dask\\array\\core.py:4806: PerformanceWarning: Increasing number of chunks by factor of 11\n",
      "  result = blockwise(\n",
      "c:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\xarray\\core\\indexing.py:1374: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 48\u001b[0m\n\u001b[0;32m     43\u001b[0m insitu_data \u001b[38;5;241m=\u001b[39m insitu_data\u001b[38;5;241m.\u001b[39mwhere(time_qc_condition, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     44\u001b[0m insitu_data \u001b[38;5;241m=\u001b[39m insitu_data\u001b[38;5;241m.\u001b[39mwhere(position_qc_condition, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 48\u001b[0m insitu_data_df \u001b[38;5;241m=\u001b[39m \u001b[43minsitu_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# convert to pandas dataframe for saving\u001b[39;00m\n\u001b[0;32m     49\u001b[0m insitu_data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m insitu_data\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# add ID variable\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(insitu_data_df)\n",
      "File \u001b[1;32mc:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\xarray\\core\\dataset.py:6201\u001b[0m, in \u001b[0;36mDataset.to_dataframe\u001b[1;34m(self, dim_order)\u001b[0m\n\u001b[0;32m   6173\u001b[0m \u001b[38;5;124;03m\"\"\"Convert this dataset into a pandas.DataFrame.\u001b[39;00m\n\u001b[0;32m   6174\u001b[0m \n\u001b[0;32m   6175\u001b[0m \u001b[38;5;124;03mNon-index variables in this dataset form the columns of the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6196\u001b[0m \n\u001b[0;32m   6197\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6199\u001b[0m ordered_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_dim_order(dim_order\u001b[38;5;241m=\u001b[39mdim_order)\n\u001b[1;32m-> 6201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mordered_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mordered_dims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\xarray\\core\\dataset.py:6165\u001b[0m, in \u001b[0;36mDataset._to_dataframe\u001b[1;34m(self, ordered_dims)\u001b[0m\n\u001b[0;32m   6163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_dataframe\u001b[39m(\u001b[38;5;28mself\u001b[39m, ordered_dims: Mapping[Any, \u001b[38;5;28mint\u001b[39m]):\n\u001b[0;32m   6164\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims]\n\u001b[1;32m-> 6165\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   6166\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variables[k]\u001b[38;5;241m.\u001b[39mset_dims(ordered_dims)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   6167\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m columns\n\u001b[0;32m   6168\u001b[0m     ]\n\u001b[0;32m   6169\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoords\u001b[38;5;241m.\u001b[39mto_index([\u001b[38;5;241m*\u001b[39mordered_dims])\n\u001b[0;32m   6170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(columns, data)), index\u001b[38;5;241m=\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\xarray\\core\\dataset.py:6166\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   6163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_dataframe\u001b[39m(\u001b[38;5;28mself\u001b[39m, ordered_dims: Mapping[Any, \u001b[38;5;28mint\u001b[39m]):\n\u001b[0;32m   6164\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims]\n\u001b[0;32m   6165\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m-> 6166\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variables\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mordered_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   6167\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m columns\n\u001b[0;32m   6168\u001b[0m     ]\n\u001b[0;32m   6169\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoords\u001b[38;5;241m.\u001b[39mto_index([\u001b[38;5;241m*\u001b[39mordered_dims])\n\u001b[0;32m   6170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(columns, data)), index\u001b[38;5;241m=\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\xarray\\core\\variable.py:608\u001b[0m, in \u001b[0;36mVariable.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;124;03m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_as_array_or_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\xarray\\core\\variable.py:314\u001b[0m, in \u001b[0;36m_as_array_or_item\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_as_array_or_item\u001b[39m(data):\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the given values as a numpy array, or as an individual item if\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03m    it's a 0d datetime64 or timedelta64 array.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m    TODO: remove this (replace with np.asarray) once these issues are fixed\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    316\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\dask\\array\\core.py:1700\u001b[0m, in \u001b[0;36mArray.__array__\u001b[1;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[0;32m   1699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 1700\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1701\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n\u001b[0;32m   1702\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mastype(dtype)\n",
      "File \u001b[1;32mc:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\dask\\base.py:315\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m    dask.base.compute\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\dask\\base.py:600\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    597\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[0;32m    598\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m--> 600\u001b[0m results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\distributed\\client.py:3122\u001b[0m, in \u001b[0;36mClient.get\u001b[1;34m(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[0;32m   3120\u001b[0m         should_rejoin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3121\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3122\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirect\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   3124\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m futures\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[1;32mc:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\distributed\\client.py:2291\u001b[0m, in \u001b[0;36mClient.gather\u001b[1;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[0;32m   2289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2290\u001b[0m     local_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gather\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2294\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2297\u001b[0m \u001b[43m    \u001b[49m\u001b[43masynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2298\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\distributed\\utils.py:339\u001b[0m, in \u001b[0;36mSyncMethodMixin.sync\u001b[1;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sync(\n\u001b[0;32m    340\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop, func, \u001b[38;5;241m*\u001b[39margs, callback_timeout\u001b[38;5;241m=\u001b[39mcallback_timeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    341\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\distributed\\utils.py:406\u001b[0m, in \u001b[0;36msync\u001b[1;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[0;32m    405\u001b[0m     typ, exc, tb \u001b[38;5;241m=\u001b[39m error\n\u001b[1;32m--> 406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\distributed\\utils.py:379\u001b[0m, in \u001b[0;36msync.<locals>.f\u001b[1;34m()\u001b[0m\n\u001b[0;32m    377\u001b[0m         future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(future, callback_timeout)\n\u001b[0;32m    378\u001b[0m     future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(future)\n\u001b[1;32m--> 379\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m future\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m     error \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[1;32mc:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\tornado\\gen.py:767\u001b[0m, in \u001b[0;36mRunner.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 767\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    769\u001b[0m         \u001b[38;5;66;03m# Save the exception for later. It's important that\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         \u001b[38;5;66;03m# gen.throw() not be called inside this try/except block\u001b[39;00m\n\u001b[0;32m    771\u001b[0m         \u001b[38;5;66;03m# because that makes sys.exc_info behave unexpectedly.\u001b[39;00m\n\u001b[0;32m    772\u001b[0m         exc: Optional[\u001b[38;5;167;01mException\u001b[39;00m] \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\distributed\\client.py:2154\u001b[0m, in \u001b[0;36mClient._gather\u001b[1;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[0;32m   2152\u001b[0m         exc \u001b[38;5;241m=\u001b[39m CancelledError(key)\n\u001b[0;32m   2153\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2154\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exception\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[0;32m   2155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m   2156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\numpy\\lib\\stride_tricks.py:413\u001b[0m, in \u001b[0;36mbroadcast_to\u001b[1;34m()\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_broadcast_to_dispatcher, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbroadcast_to\u001b[39m(array, shape, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;124;03m\"\"\"Broadcast an array to a new shape.\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;124;03m           [1, 2, 3]])\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _broadcast_to(array, shape, subok\u001b[38;5;241m=\u001b[39msubok, readonly\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\s1834371\\Miniconda3\\envs\\JIF_main\\lib\\site-packages\\numpy\\lib\\stride_tricks.py:354\u001b[0m, in \u001b[0;36m_broadcast_to\u001b[1;34m()\u001b[0m\n\u001b[0;32m    349\u001b[0m it \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnditer(\n\u001b[0;32m    350\u001b[0m     (array,), flags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti_index\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrefs_ok\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzerosize_ok\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m extras,\n\u001b[0;32m    351\u001b[0m     op_flags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadonly\u001b[39m\u001b[38;5;124m'\u001b[39m], itershape\u001b[38;5;241m=\u001b[39mshape, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m it:\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;66;03m# never really has writebackifcopy semantics\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     broadcast \u001b[38;5;241m=\u001b[39m it\u001b[38;5;241m.\u001b[39mitviews[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    355\u001b[0m result \u001b[38;5;241m=\u001b[39m _maybe_view_as_subclass(array, broadcast)\n\u001b[0;32m    356\u001b[0m \u001b[38;5;66;03m# In a future version this will go away\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 19:55:19,604 - distributed.scheduler - WARNING - Worker failed to heartbeat within 300 seconds. Closing: <WorkerState 'tcp://127.0.0.1:52355', name: 0, status: running, memory: 0, processing: 0>\n",
      "2023-12-15 19:55:19,730 - distributed.scheduler - WARNING - Worker failed to heartbeat within 300 seconds. Closing: <WorkerState 'tcp://127.0.0.1:51428', name: 3, status: running, memory: 0, processing: 0>\n",
      "2023-12-15 19:55:19,736 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:52355'.\n",
      "2023-12-15 19:55:20,003 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:51428'.\n",
      "2023-12-15 19:55:22,384 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask.diagnostics import ProgressBar\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "client = Client(n_workers=4, threads_per_worker=2, memory_limit='8GB')\n",
    "\n",
    "parent_data_dir = r'C:/Users/s1834371/Documents/PLUMES/validation/mooring_test_4/'  #file directory of in-situ data\n",
    "\n",
    "# Get list of .nc files\n",
    "file_list = glob.glob(parent_data_dir + '**/*.nc', recursive=True)\n",
    "\n",
    "for file in file_list:\n",
    "    print('\\n====================')\n",
    "    print('filename:', os.path.basename(file))\n",
    "\n",
    "    insitu_data = xr.open_dataset(file)  # open in-situ data\n",
    "    chunk_size = int(len(insitu_data.TIME) / 10)\n",
    "\n",
    "    insitu_data = insitu_data.chunk({'TIME':chunk_size,'LONGITUDE': chunk_size, 'LATITUDE':chunk_size, 'POSITION':chunk_size})  # chunks need to be as big as possible\n",
    "\n",
    "    insitu_data = insitu_data.isel(DEPTH=0) # select time period of interest\n",
    "    insitu_data = insitu_data[['TEMP', 'TEMP_QC', 'DEPH', 'DEPH_QC', 'TIME_QC', 'POSITION_QC', 'LONGITUDE', 'LATITUDE', 'TIME', 'POSITION']]\n",
    "\n",
    "\n",
    "    print(insitu_data)\n",
    "\n",
    "    depth_condition = (insitu_data['DEPH'] >= 0.0) & (insitu_data['DEPH'] < 3.0) \n",
    "    temp_qc_condition = (insitu_data['TEMP_QC'] == 1)\n",
    "    deph_qc_condition = (insitu_data['DEPH_QC'] == 1) | (insitu_data['DEPH_QC'] == 7)\n",
    "    time_qc_condition = (insitu_data['TIME_QC'] <= 2)\n",
    "    position_qc_condition = (insitu_data['POSITION_QC'] <= 2)\n",
    "\n",
    "    # Setting conditions for dataset\n",
    "    insitu_data = insitu_data.where(depth_condition, drop=True)\n",
    "    insitu_data = insitu_data.where(temp_qc_condition, drop=True)\n",
    "    insitu_data = insitu_data.where(deph_qc_condition, drop=True)\n",
    "    insitu_data = insitu_data.where(time_qc_condition, drop=True)\n",
    "    insitu_data = insitu_data.where(position_qc_condition, drop=True)\n",
    "\n",
    "    insitu_data_df = insitu_data.to_dataframe() # convert to pandas dataframe for saving\n",
    "    insitu_data_df['ID'] = insitu_data.attrs['id'] # add ID variable\n",
    "    print(insitu_data_df)\n",
    "    insitu_data_df.to_csv(file[:-3] + '_surface_v3.csv') # save as csv\n",
    "\n",
    "    print('\\n AFTER FILTERING')\n",
    "    print(insitu_data_df)\n",
    "    sys.exit()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GFSTS_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
